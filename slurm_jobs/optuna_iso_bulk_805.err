/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:35:03.745351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:35:04.736001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:35:07.030177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:35:09.544632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:35:10,061 - INFO - Using device: cuda
2025-11-21 01:35:10,063 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:10,065 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:10,065 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
2025-11-21 01:35:10,072 - INFO - Using device: cuda
2025-11-21 01:35:10,073 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:10,074 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:10,074 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[I 2025-11-21 01:35:11,013] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:35:11,028] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:35:11.492671: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:35:11,755 - INFO - Using device: cuda
2025-11-21 01:35:11,756 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:11,757 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:11,757 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:35:12,555] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:35:13.229619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:35:14,151 - INFO - Using device: cuda
2025-11-21 01:35:14,153 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:14,153 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:14,153 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:35:14,962] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:35:15.292123: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:35:16,085 - INFO - Using device: cuda
2025-11-21 01:35:16,087 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:16,087 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:16,087 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:35:16,837] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:35:17,836 - INFO - Using device: cuda
2025-11-21 01:35:17,837 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:17,838 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:17,838 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:35:18,607] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:35:19,800 - INFO - Using device: cuda
2025-11-21 01:35:19,802 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:35:19,802 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:35:19,802 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:35:20,558] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:46:29,034] Trial 3182 pruned. 
[I 2025-11-21 01:47:06,201] Trial 3176 pruned. 
[I 2025-11-21 01:47:47,456] Trial 3191 pruned. 
[I 2025-11-21 01:47:56,601] Trial 3196 pruned. 
[I 2025-11-21 01:48:16,177] Trial 3180 pruned. 
[I 2025-11-21 01:48:49,156] Trial 3194 pruned. 
[I 2025-11-21 01:48:50,543] Trial 3197 pruned. 
[I 2025-11-21 01:49:36,330] Trial 3184 pruned. 
[I 2025-11-21 01:50:14,006] Trial 3200 pruned. 
[I 2025-11-21 01:50:25,815] Trial 3185 pruned. 
[I 2025-11-21 01:50:53,256] Trial 3188 pruned. 
[I 2025-11-21 01:51:04,214] Trial 3193 pruned. 
[I 2025-11-21 01:51:04,583] Trial 3198 pruned. 
[I 2025-11-21 01:51:07,690] Trial 3201 pruned. 
[I 2025-11-21 01:51:33,256] Trial 3190 pruned. 
[I 2025-11-21 01:51:38,774] Trial 3178 pruned. 
[I 2025-11-21 01:52:02,690] Trial 3189 pruned. 
[I 2025-11-21 01:53:41,828] Trial 3186 pruned. 
[I 2025-11-21 01:55:48,085] Trial 3183 pruned. 
[I 2025-11-21 01:55:50,229] Trial 3187 pruned. 
[I 2025-11-21 01:56:48,570] Trial 3175 pruned. 
[I 2025-11-21 01:59:08,033] Trial 3179 pruned. 
[I 2025-11-21 01:59:34,246] Trial 3209 pruned. 
[I 2025-11-21 01:59:34,961] Trial 3181 pruned. 
[I 2025-11-21 02:00:30,081] Trial 3210 pruned. 
[I 2025-11-21 02:01:10,642] Trial 3211 pruned. 
[I 2025-11-21 02:02:25,851] Trial 3195 pruned. 
[I 2025-11-21 02:02:26,583] Trial 3217 pruned. 
[I 2025-11-21 02:02:35,389] Trial 3213 pruned. 
[I 2025-11-21 02:02:57,997] Trial 3212 pruned. 
[I 2025-11-21 02:03:23,198] Trial 3216 pruned. 
[I 2025-11-21 02:03:39,156] Trial 3214 pruned. 
[I 2025-11-21 02:03:40,792] Trial 3199 pruned. 
[I 2025-11-21 02:03:42,307] Trial 3215 pruned. 
[I 2025-11-21 02:03:53,401] Trial 3220 pruned. 
[I 2025-11-21 02:04:12,187] Trial 3218 pruned. 
[I 2025-11-21 02:04:26,367] Trial 3202 pruned. 
[I 2025-11-21 02:05:23,493] Trial 3219 pruned. 
[I 2025-11-21 02:05:39,257] Trial 3221 pruned. 
[I 2025-11-21 02:06:24,022] Trial 3222 pruned. 
[I 2025-11-21 02:09:06,541] Trial 3223 pruned. 
[I 2025-11-21 02:09:06,565] Trial 3224 pruned. 
[I 2025-11-21 02:09:50,575] Trial 3226 pruned. 
[I 2025-11-21 02:10:21,103] Trial 3225 pruned. 
[I 2025-11-21 02:11:14,274] Trial 3192 pruned. 
[I 2025-11-21 02:11:23,338] Trial 3227 pruned. 
[I 2025-11-21 02:12:00,616] Trial 3228 pruned. 
[I 2025-11-21 02:12:20,419] Trial 3229 pruned. 
[I 2025-11-21 02:13:18,581] Trial 3230 pruned. 
[I 2025-11-21 02:13:29,149] Trial 3235 pruned. 
[I 2025-11-21 02:13:55,494] Trial 3237 pruned. 
[I 2025-11-21 02:14:07,861] Trial 3231 pruned. 
[I 2025-11-21 02:15:32,011] Trial 3232 pruned. 
[I 2025-11-21 02:15:59,147] Trial 3233 pruned. 
[I 2025-11-21 02:16:15,791] Trial 3234 pruned. 
[I 2025-11-21 02:16:18,322] Trial 3236 pruned. 
[I 2025-11-21 02:20:06,346] Trial 3245 pruned. 
[I 2025-11-21 02:21:09,996] Trial 3247 pruned. 
[I 2025-11-21 02:21:21,206] Trial 3243 pruned. 
[I 2025-11-21 02:22:10,421] Trial 3248 pruned. 
[I 2025-11-21 02:24:30,162] Trial 3206 pruned. 
[I 2025-11-21 02:25:08,976] Trial 3204 pruned. 
[I 2025-11-21 02:25:11,433] Trial 3207 pruned. 
[I 2025-11-21 02:26:01,463] Trial 3208 pruned. 
[I 2025-11-21 02:28:08,836] Trial 3257 pruned. 
[I 2025-11-21 02:29:54,431] Trial 3205 pruned. 
[I 2025-11-21 02:32:53,350] Trial 3261 pruned. 
[I 2025-11-21 02:35:46,380] Trial 3265 pruned. 
[I 2025-11-21 02:38:15,938] Trial 3264 pruned. 
[I 2025-11-21 02:39:58,925] Trial 3268 pruned. 
[I 2025-11-21 02:42:49,409] Trial 3255 pruned. 
[I 2025-11-21 02:43:03,185] Trial 3259 pruned. 
[I 2025-11-21 02:43:17,502] Trial 3253 pruned. 
[I 2025-11-21 02:43:45,703] Trial 3239 pruned. 
[I 2025-11-21 02:46:36,532] Trial 3270 pruned. 
[I 2025-11-21 02:48:15,433] Trial 3263 pruned. 
[I 2025-11-21 02:48:18,282] Trial 3271 pruned. 
[I 2025-11-21 02:50:26,201] Trial 3251 pruned. 
[I 2025-11-21 02:51:34,943] Trial 3266 pruned. 
[I 2025-11-21 02:53:51,298] Trial 3256 pruned. 
[I 2025-11-21 02:54:51,994] Trial 3203 finished with value: 0.002950531430542469 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0008583137609127937, 'nr_neurons': 224, 'dropout_rate': 0.026941066620796074, 'weight_decay': 1.5762332746685456e-06, 'rlr_factor': 0.18646028540158854, 'rlr_patience': 7, 'rlr_min_lr': 1.7080947205970118e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 35, 'robust_q_high': 75, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 02:58:12,407] Trial 3242 pruned. 
[I 2025-11-21 02:58:55,804] Trial 3238 pruned. 
[I 2025-11-21 02:59:02,855] Trial 3278 pruned. 
[I 2025-11-21 03:00:10,012] Trial 3254 pruned. 
[I 2025-11-21 03:00:35,609] Trial 3249 pruned. 
[I 2025-11-21 03:04:10,059] Trial 3267 pruned. 
[I 2025-11-21 03:05:23,708] Trial 3283 pruned. 
[I 2025-11-21 03:08:00,157] Trial 3286 pruned. 
[I 2025-11-21 03:08:09,212] Trial 3241 pruned. 
[I 2025-11-21 03:09:38,979] Trial 3284 pruned. 
[I 2025-11-21 03:10:39,967] Trial 3282 pruned. 
[I 2025-11-21 03:11:06,562] Trial 3288 pruned. 
[I 2025-11-21 03:12:21,857] Trial 3287 pruned. 
[I 2025-11-21 03:18:17,001] Trial 3289 pruned. 
[I 2025-11-21 03:19:41,188] Trial 3269 pruned. 
[I 2025-11-21 03:20:24,869] Trial 3252 pruned. 
[I 2025-11-21 03:23:06,099] Trial 3272 pruned. 
[I 2025-11-21 03:23:42,075] Trial 3177 pruned. 
[I 2025-11-21 03:25:03,223] Trial 3275 pruned. 
[I 2025-11-21 03:28:56,168] Trial 3293 pruned. 
[I 2025-11-21 03:29:32,610] Trial 3279 pruned. 
[I 2025-11-21 03:30:40,401] Trial 3280 pruned. 
[I 2025-11-21 03:49:01,850] Trial 3292 pruned. 
[I 2025-11-21 03:57:59,398] Trial 3297 pruned. 
[I 2025-11-21 03:58:38,539] Trial 3306 pruned. 
[I 2025-11-21 04:00:22,783] Trial 3294 pruned. 
[I 2025-11-21 04:02:11,889] Trial 3301 pruned. 
[I 2025-11-21 04:02:35,421] Trial 3302 pruned. 
[I 2025-11-21 04:03:44,199] Trial 3305 pruned. 
[I 2025-11-21 04:06:35,731] Trial 3246 pruned. 
[I 2025-11-21 04:06:37,082] Trial 3285 pruned. 
[I 2025-11-21 04:12:05,122] Trial 3250 pruned. 
[I 2025-11-21 04:16:29,998] Trial 3276 pruned. 
[I 2025-11-21 04:18:19,013] Trial 3314 pruned. 
[I 2025-11-21 04:18:29,341] Trial 3290 pruned. 
[I 2025-11-21 04:21:36,278] Trial 3262 pruned. 
[I 2025-11-21 04:29:21,944] Trial 3299 pruned. 
[I 2025-11-21 04:35:18,105] Trial 3308 pruned. 
[I 2025-11-21 04:37:03,007] Trial 3244 pruned. 
[I 2025-11-21 04:40:00,427] Trial 3312 pruned. 
[I 2025-11-21 04:41:10,164] Trial 3311 pruned. 
[I 2025-11-21 04:44:38,254] Trial 3309 pruned. 
[I 2025-11-21 04:44:38,741] Trial 3291 pruned. 
[I 2025-11-21 04:45:02,159] Trial 3321 pruned. 
[I 2025-11-21 04:47:08,287] Trial 3322 pruned. 
[I 2025-11-21 04:50:09,600] Trial 3316 pruned. 
[I 2025-11-21 04:50:16,560] Trial 3315 pruned. 
[I 2025-11-21 04:50:16,587] Trial 3258 pruned. 
[I 2025-11-21 04:52:21,433] Trial 3300 pruned. 
[I 2025-11-21 04:54:31,970] Trial 3326 pruned. 
[I 2025-11-21 04:54:42,449] Trial 3327 pruned. 
[I 2025-11-21 04:55:58,202] Trial 3320 pruned. 
[I 2025-11-21 04:57:04,370] Trial 3325 pruned. 
[I 2025-11-21 04:57:06,508] Trial 3328 pruned. 
[I 2025-11-21 05:00:08,204] Trial 3329 pruned. 
[I 2025-11-21 05:00:36,390] Trial 3330 pruned. 
[I 2025-11-21 05:02:38,873] Trial 3332 pruned. 
[I 2025-11-21 05:04:08,290] Trial 3318 pruned. 
[I 2025-11-21 05:05:04,509] Trial 3277 pruned. 
[I 2025-11-21 05:07:06,441] Trial 3337 pruned. 
[I 2025-11-21 05:08:13,182] Trial 3295 pruned. 
[I 2025-11-21 05:09:29,935] Trial 3296 pruned. 
[I 2025-11-21 05:10:46,288] Trial 3339 pruned. 
[I 2025-11-21 05:13:08,846] Trial 3323 pruned. 
[I 2025-11-21 05:13:08,865] Trial 3303 pruned. 
[I 2025-11-21 05:14:21,995] Trial 3341 pruned. 
[I 2025-11-21 05:15:57,495] Trial 3342 pruned. 
[I 2025-11-21 05:20:56,785] Trial 3304 pruned. 
[I 2025-11-21 05:26:58,019] Trial 3340 pruned. 
[I 2025-11-21 05:29:24,538] Trial 3298 pruned. 
[I 2025-11-21 05:30:07,905] Trial 3334 pruned. 
[I 2025-11-21 05:39:57,462] Trial 3354 pruned. 
[I 2025-11-21 05:42:03,914] Trial 3353 pruned. 
[I 2025-11-21 05:43:01,719] Trial 3335 pruned. 
[I 2025-11-21 05:44:45,807] Trial 3343 pruned. 
[I 2025-11-21 05:46:32,569] Trial 3331 pruned. 
[I 2025-11-21 05:49:31,835] Trial 3338 pruned. 
[I 2025-11-21 05:49:46,085] Trial 3346 pruned. 
[I 2025-11-21 05:50:13,560] Trial 3345 pruned. 
[I 2025-11-21 05:51:46,270] Trial 3281 pruned. 
[I 2025-11-21 05:52:32,434] Trial 3347 pruned. 
[I 2025-11-21 05:53:16,938] Trial 3336 pruned. 
[I 2025-11-21 05:57:30,876] Trial 3349 pruned. 
[I 2025-11-21 05:59:39,661] Trial 3360 pruned. 
[I 2025-11-21 05:59:51,598] Trial 3348 pruned. 
[I 2025-11-21 06:02:08,938] Trial 3350 pruned. 
[I 2025-11-21 06:02:23,218] Trial 3351 pruned. 
[I 2025-11-21 06:06:08,816] Trial 3313 pruned. 
[I 2025-11-21 06:08:31,598] Trial 3344 pruned. 
[I 2025-11-21 06:13:16,750] Trial 3357 pruned. 
[I 2025-11-21 06:13:45,101] Trial 3355 pruned. 
[I 2025-11-21 06:15:03,184] Trial 3370 pruned. 
[I 2025-11-21 06:15:30,991] Trial 3362 pruned. 
[I 2025-11-21 06:16:51,422] Trial 3366 pruned. 
[I 2025-11-21 06:17:07,379] Trial 3352 pruned. 
[I 2025-11-21 06:17:14,852] Trial 3368 pruned. 
[I 2025-11-21 06:18:56,450] Trial 3367 pruned. 
[I 2025-11-21 06:20:40,303] Trial 3358 pruned. 
[I 2025-11-21 06:20:46,231] Trial 3359 pruned. 
[I 2025-11-21 06:23:22,216] Trial 3361 pruned. 
[I 2025-11-21 06:23:31,570] Trial 3374 pruned. 
[I 2025-11-21 06:24:03,409] Trial 3365 pruned. 
[I 2025-11-21 06:24:29,261] Trial 3324 pruned. 
[I 2025-11-21 06:29:27,545] Trial 3371 pruned. 
[I 2025-11-21 06:31:07,795] Trial 3319 pruned. 
[I 2025-11-21 06:31:07,860] Trial 3363 pruned. 
[I 2025-11-21 06:31:13,103] Trial 3356 pruned. 
[I 2025-11-21 06:32:26,126] Trial 3364 pruned. 
[I 2025-11-21 06:33:16,273] Trial 3384 pruned. 
[I 2025-11-21 06:33:53,128] Trial 3372 pruned. 
[I 2025-11-21 06:34:23,974] Trial 3386 pruned. 
[I 2025-11-21 06:36:29,791] Trial 3385 pruned. 
[I 2025-11-21 06:37:52,779] Trial 3369 pruned. 
[I 2025-11-21 06:37:55,343] Trial 3376 pruned. 
[I 2025-11-21 06:40:17,749] Trial 3375 pruned. 
[I 2025-11-21 06:42:01,055] Trial 3389 pruned. 
[I 2025-11-21 06:43:51,311] Trial 3378 pruned. 
[I 2025-11-21 06:48:42,167] Trial 3397 pruned. 
[I 2025-11-21 06:48:46,179] Trial 3396 pruned. 
[I 2025-11-21 06:49:16,270] Trial 3381 pruned. 
[I 2025-11-21 06:50:13,989] Trial 3398 pruned. 
[I 2025-11-21 06:50:24,525] Trial 3395 pruned. 
[I 2025-11-21 06:52:57,527] Trial 3399 pruned. 
[I 2025-11-21 06:53:02,471] Trial 3383 pruned. 
[I 2025-11-21 06:54:06,585] Trial 3400 pruned. 
[I 2025-11-21 06:54:20,410] Trial 3380 pruned. 
[I 2025-11-21 06:56:10,502] Trial 3388 pruned. 
[I 2025-11-21 06:59:27,498] Trial 3401 pruned. 
[I 2025-11-21 07:00:10,195] Trial 3404 pruned. 
[I 2025-11-21 07:00:18,916] Trial 3394 pruned. 
[I 2025-11-21 07:01:57,225] Trial 3382 pruned. 
[I 2025-11-21 07:14:57,770] Trial 3393 pruned. 
[I 2025-11-21 07:25:25,571] Trial 3409 pruned. 
[I 2025-11-21 07:29:23,748] Trial 3391 pruned. 
[I 2025-11-21 07:32:03,771] Trial 3410 pruned. 
[I 2025-11-21 07:34:12,255] Trial 3412 pruned. 
[I 2025-11-21 07:35:01,427] Trial 3403 pruned. 
[I 2025-11-21 07:39:22,172] Trial 3408 pruned. 
[I 2025-11-21 07:42:15,112] Trial 3402 pruned. 
[I 2025-11-21 07:43:04,518] Trial 3418 pruned. 
[I 2025-11-21 07:43:30,419] Trial 3310 finished with value: 0.0005294137517921627 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0011899852552049637, 'nr_neurons': 245, 'dropout_rate': 0.0002804295524850485, 'weight_decay': 1.6425303807435256e-06, 'rlr_factor': 0.7470540156974291, 'rlr_patience': 7, 'rlr_min_lr': 1.2425586386733699e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 35, 'robust_q_high': 73, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 07:44:08,282] Trial 3414 pruned. 
[I 2025-11-21 07:44:19,090] Trial 3405 pruned. 
[I 2025-11-21 07:53:43,065] Trial 3424 pruned. 
[I 2025-11-21 07:54:03,660] Trial 3423 pruned. 
[I 2025-11-21 07:54:08,115] Trial 3422 pruned. 
[I 2025-11-21 08:00:27,906] Trial 3426 pruned. 
[I 2025-11-21 08:02:07,057] Trial 3420 pruned. 
[I 2025-11-21 08:02:36,116] Trial 3416 pruned. 
[I 2025-11-21 08:07:01,571] Trial 3373 pruned. 
[I 2025-11-21 08:12:14,420] Trial 3431 pruned. 
[I 2025-11-21 08:12:36,759] Trial 3432 pruned. 
[I 2025-11-21 08:13:16,425] Trial 3430 pruned. 
[I 2025-11-21 08:17:20,450] Trial 3421 pruned. 
[I 2025-11-21 08:17:53,345] Trial 3260 finished with value: 0.00043446096242405474 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0010902571105971977, 'nr_neurons': 211, 'dropout_rate': 0.00014886148357878026, 'weight_decay': 1.1980934452645068e-06, 'rlr_factor': 0.8197518696380568, 'rlr_patience': 7, 'rlr_min_lr': 1.509194209608254e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 33, 'robust_q_high': 74, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 08:20:50,389] Trial 3274 finished with value: 0.0004108286229893565 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0008065164766789312, 'nr_neurons': 256, 'dropout_rate': 7.973068701354483e-05, 'weight_decay': 1.33515534230752e-06, 'rlr_factor': 0.8044585089325013, 'rlr_patience': 7, 'rlr_min_lr': 1.2262274993426781e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 33, 'robust_q_high': 74, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 08:22:20,368] Trial 3434 pruned. 
[I 2025-11-21 08:26:03,865] Trial 3427 pruned. 
[I 2025-11-21 08:27:37,071] Trial 3437 pruned. 
[I 2025-11-21 08:34:36,237] Trial 3429 pruned. 
[I 2025-11-21 08:37:53,131] Trial 3442 pruned. 
[I 2025-11-21 08:42:20,967] Trial 3433 pruned. 
[I 2025-11-21 08:43:13,867] Trial 3436 pruned. 
[I 2025-11-21 08:43:56,714] Trial 3392 pruned. 
[I 2025-11-21 08:45:38,243] Trial 3443 pruned. 
[I 2025-11-21 08:47:40,171] Trial 3425 pruned. 
[I 2025-11-21 08:49:46,973] Trial 3415 pruned. 
[I 2025-11-21 08:52:26,766] Trial 3439 pruned. 
[I 2025-11-21 08:52:35,696] Trial 3445 pruned. 
[I 2025-11-21 08:54:56,179] Trial 3413 pruned. 
[I 2025-11-21 08:58:45,468] Trial 3440 pruned. 
[I 2025-11-21 09:02:57,615] Trial 3390 pruned. 
[I 2025-11-21 09:05:10,751] Trial 3449 pruned. 
[I 2025-11-21 09:05:21,741] Trial 3441 pruned. 
[I 2025-11-21 09:05:51,404] Trial 3453 pruned. 
[I 2025-11-21 09:07:23,562] Trial 3406 pruned. 
[I 2025-11-21 09:10:53,778] Trial 3317 finished with value: 0.0005008237203583121 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0009040130530991711, 'nr_neurons': 233, 'dropout_rate': 0.00016957875939435025, 'weight_decay': 1.6954789887164047e-06, 'rlr_factor': 0.753635488774982, 'rlr_patience': 7, 'rlr_min_lr': 1.8794672577205465e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 35, 'robust_q_high': 78, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 09:11:51,309] Trial 3451 pruned. 
[I 2025-11-21 09:14:03,102] Trial 3444 pruned. 
[I 2025-11-21 09:15:35,185] Trial 3307 finished with value: 0.00048314151354134083 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0011510237442418747, 'nr_neurons': 244, 'dropout_rate': 0.00020702107899349333, 'weight_decay': 1.673118082690102e-06, 'rlr_factor': 0.752849207131221, 'rlr_patience': 7, 'rlr_min_lr': 1.9648339826638918e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 35, 'robust_q_high': 73, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 09:15:47,602] Trial 3458 pruned. 
[I 2025-11-21 09:17:53,521] Trial 3456 pruned. 
[I 2025-11-21 09:19:22,821] Trial 3273 finished with value: 0.00048412205069325864 and parameters: {'batch_size': 32, 'nr_hidden_layers': 4, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.001153643405876155, 'nr_neurons': 247, 'dropout_rate': 0.00019249291982573874, 'weight_decay': 1.3286333095293578e-06, 'rlr_factor': 0.7983965748928025, 'rlr_patience': 7, 'rlr_min_lr': 1.2221563756237057e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 33, 'robust_q_high': 74, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 09:23:19,760] Trial 3460 pruned. 
[I 2025-11-21 09:25:42,817] Trial 3435 pruned. 
[I 2025-11-21 09:25:45,546] Trial 3461 pruned. 
[I 2025-11-21 09:25:53,063] Trial 3464 pruned. 
[I 2025-11-21 09:27:28,396] Trial 3450 pruned. 
[I 2025-11-21 09:29:00,878] Trial 3455 pruned. 
[I 2025-11-21 09:29:09,488] Trial 3446 pruned. 
[I 2025-11-21 09:30:41,062] Trial 3465 pruned. 
[I 2025-11-21 09:32:07,910] Trial 3466 pruned. 
[I 2025-11-21 09:36:10,067] Trial 3470 pruned. 
[I 2025-11-21 09:36:11,346] Trial 3454 pruned. 
[I 2025-11-21 09:36:24,796] Trial 3463 pruned. 
[I 2025-11-21 09:36:44,292] Trial 3469 pruned. 
[I 2025-11-21 09:38:29,100] Trial 3468 pruned. 
[I 2025-11-21 09:38:30,632] Trial 3471 pruned. 
[I 2025-11-21 09:38:57,464] Trial 3457 pruned. 
[I 2025-11-21 09:40:09,670] Trial 3462 pruned. 
[I 2025-11-21 09:51:57,412] Trial 3482 pruned. 
[I 2025-11-21 09:53:12,834] Trial 3483 pruned. 
[I 2025-11-21 10:03:15,382] Trial 3438 pruned. 
[I 2025-11-21 10:03:28,993] Trial 3417 pruned. 
[I 2025-11-21 10:04:22,443] Trial 3481 pruned. 
[I 2025-11-21 10:08:12,802] Trial 3476 pruned. 
[I 2025-11-21 10:08:50,136] Trial 3419 pruned. 
[I 2025-11-21 10:09:38,087] Trial 3448 pruned. 
[I 2025-11-21 10:12:12,618] Trial 3475 pruned. 
[I 2025-11-21 10:13:06,055] Trial 3478 pruned. 
[I 2025-11-21 10:18:54,534] Trial 3490 pruned. 
[I 2025-11-21 10:20:22,639] Trial 3472 pruned. 
[I 2025-11-21 10:20:40,712] Trial 3491 pruned. 
[I 2025-11-21 10:21:36,984] Trial 3484 pruned. 
[I 2025-11-21 10:24:50,732] Trial 3492 pruned. 
[I 2025-11-21 10:25:52,562] Trial 3493 pruned. 
[I 2025-11-21 10:28:57,233] Trial 3494 pruned. 
[I 2025-11-21 10:30:13,086] Trial 3488 pruned. 
[I 2025-11-21 10:31:32,203] Trial 3496 pruned. 
[I 2025-11-21 10:33:01,279] Trial 3495 pruned. 
[I 2025-11-21 10:36:51,928] Trial 3487 pruned. 
[I 2025-11-21 10:37:34,963] Trial 3498 pruned. 
[I 2025-11-21 10:38:39,659] Trial 3333 finished with value: 0.0004444278310984373 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0008311807649129825, 'nr_neurons': 219, 'dropout_rate': 2.3860278506998244e-05, 'weight_decay': 1.3893599436665682e-06, 'rlr_factor': 0.7769185220806075, 'rlr_patience': 7, 'rlr_min_lr': 1.0041173815429791e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 32, 'robust_q_high': 73, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 10:42:30,122] Trial 3452 finished with value: 0.001253448543138802 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'SmoothL1', 'learning_rate': 0.0006187663191475546, 'nr_neurons': 200, 'dropout_rate': 6.423409287072204e-05, 'weight_decay': 1.1160514095766543e-06, 'rlr_factor': 0.8270225501584512, 'rlr_patience': 7, 'rlr_min_lr': 1.2271457030607697e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 32, 'robust_q_high': 76, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
[I 2025-11-21 10:43:14,163] Trial 3447 pruned. 
[I 2025-11-21 10:44:19,480] Trial 3485 pruned. 
[I 2025-11-21 10:45:58,972] Trial 3486 pruned. 
[I 2025-11-21 10:46:07,794] Trial 3489 pruned. 
[I 2025-11-21 10:49:07,641] Trial 3407 finished with value: 0.00045154840336181223 and parameters: {'batch_size': 32, 'nr_hidden_layers': 3, 'activation_name': 'GELU', 'loss_criterion': 'L1', 'learning_rate': 0.0009123997796815089, 'nr_neurons': 244, 'dropout_rate': 0.00012515612275241014, 'weight_decay': 1.1772968912233956e-06, 'rlr_factor': 0.761458901780573, 'rlr_patience': 7, 'rlr_min_lr': 1.0042356068038921e-06, 'feature_scaler': 'robust', 'label_scaler': 'minmax', 'minmax_label_signed': False, 'robust_q_low': 34, 'robust_q_high': 73, 'use_batchnorm': False}. Best is trial 2824 with value: 0.000382560450816527.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
slurmstepd-argon-gtx: error: *** STEP 805.6 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.5 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.3 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.2 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.1 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.0 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** JOB 805 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
slurmstepd-argon-gtx: error: *** STEP 805.4 ON argon-gtx CANCELLED AT 2025-11-21T10:50:15 ***
