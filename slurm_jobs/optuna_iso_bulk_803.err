/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:20:41.063200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:20:41.063185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:20:41.063186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:20:42.629273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/barattts/lavoltabuona/BA/.venv/daicheelavoltabuona/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-21 01:20:44.695732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:20:46.553435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-21 01:20:48,589 - INFO - Using device: cuda
2025-11-21 01:20:48,591 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:48,592 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:48,592 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
2025-11-21 01:20:48,619 - INFO - Using device: cuda
2025-11-21 01:20:48,622 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:48,624 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:48,624 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
2025-11-21 01:20:48,625 - INFO - Using device: cuda
2025-11-21 01:20:48,627 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:48,628 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:48,628 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
2025-11-21 01:20:48,636 - INFO - Using device: cuda
2025-11-21 01:20:48,639 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:48,640 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:48,641 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
2025-11-21 01:20:48.755953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[I 2025-11-21 01:20:49,684] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:20:49,690] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:20:49,694] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:20:49,696] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:20:50,096 - INFO - Using device: cuda
2025-11-21 01:20:50,098 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:50,098 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:50,098 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:20:50,895] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:20:51,529 - INFO - Using device: cuda
2025-11-21 01:20:51,531 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:51,532 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:51,532 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:20:52,330] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
2025-11-21 01:20:53,132 - INFO - Using device: cuda
2025-11-21 01:20:53,133 - INFO - Target labels for this run: ['Area', 'Iso_distance', 'Iso_width']
2025-11-21 01:20:53,134 - INFO - Optuna config -> study: nn_study_isotherm_bulk | storage: sqlite:///runs/optuna_isotherm_bulk.db | trials: 1429 | n_jobs: 4
2025-11-21 01:20:53,134 - INFO - Starting Optuna study: nn_study_isotherm_bulk...
[I 2025-11-21 01:20:53,969] Using an existing study with name 'nn_study_isotherm_bulk' instead of creating a new one.
[I 2025-11-21 01:27:37,293] Trial 3129 pruned. 
[I 2025-11-21 01:27:37,388] Trial 3121 pruned. 
[I 2025-11-21 01:27:37,854] Trial 3124 pruned. 
[I 2025-11-21 01:27:38,125] Trial 3133 pruned. 
[I 2025-11-21 01:27:39,848] Trial 3117 pruned. 
[I 2025-11-21 01:27:40,559] Trial 3138 pruned. 
[I 2025-11-21 01:27:42,110] Trial 3140 pruned. 
[I 2025-11-21 01:27:48,570] Trial 3132 pruned. 
[I 2025-11-21 01:27:49,335] Trial 3137 pruned. 
[I 2025-11-21 01:27:49,809] Trial 3142 pruned. 
[I 2025-11-21 01:27:49,881] Trial 3126 pruned. 
[I 2025-11-21 01:27:51,636] Trial 3130 pruned. 
[I 2025-11-21 01:27:51,964] Trial 3118 pruned. 
[I 2025-11-21 01:28:08,998] Trial 3125 pruned. 
[I 2025-11-21 01:28:09,394] Trial 3120 pruned. 
[I 2025-11-21 01:28:11,011] Trial 3134 pruned. 
[I 2025-11-21 01:28:11,333] Trial 3116 pruned. 
[I 2025-11-21 01:28:12,373] Trial 3139 pruned. 
[I 2025-11-21 01:28:13,545] Trial 3143 pruned. 
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
slurmstepd-argon-gtx: error: *** STEP 803.0 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.3 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.2 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.1 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.6 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.4 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** JOB 803 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
slurmstepd-argon-gtx: error: *** STEP 803.5 ON argon-gtx CANCELLED AT 2025-11-21T01:28:40 ***
